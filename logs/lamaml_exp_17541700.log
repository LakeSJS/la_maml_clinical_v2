============================================
Job ID: 17541700
Job Name: lamaml_exp
Node: a100-4007
Partition: a100_short
Start time: Wed Jan 21 15:15:15 EST 2026
Config: traditional_nonseq_2013_2018
Seed: 10
Paths: gpfs
============================================
Running: python /gpfs/data/oermannlab/users/slj9342/la_maml_clinical_v2/scripts/run_experiment.py --config traditional_nonseq_2013_2018 --paths gpfs --seed 10
============================================
[rank: 0] Global seed set to 10
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /gpfs/data/oermannlab/NYUTron/model_zoos/nyutron_small/checkpoint-736000 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: WARNING `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id enssektn.
wandb: Tracking run with wandb version 0.19.8
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name     | Type                          | Params
-----------------------------------------------------------
0 | model    | BertForSequenceClassification | 124 M 
1 | val_auc  | BinaryAUROC                   | 0     
2 | test_auc | BinaryAUROC                   | 0     
-----------------------------------------------------------
124 M     Trainable params
0         Non-trainable params
124 M     Total params
497.772   Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
/gpfs/data/oermannlab/users/slj9342/.conda/envs/amazon_fashion_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/gpfs/data/oermannlab/users/slj9342/.conda/envs/amazon_fashion_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
`Trainer.fit` stopped: `max_epochs=40` reached.
Restoring states from the checkpoint path at /gpfs/data/oermannlab/users/slj9342/la_maml_clinical_v2/checkpoints/la_maml_clinical_v2/traditional-nonsequential-2013-2018/seed-10/best-checkpoint.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded model weights from the checkpoint at /gpfs/data/oermannlab/users/slj9342/la_maml_clinical_v2/checkpoints/la_maml_clinical_v2/traditional-nonsequential-2013-2018/seed-10/best-checkpoint.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
/gpfs/data/oermannlab/users/slj9342/.conda/envs/amazon_fashion_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃ Runningstage.tes… ┃                   ┃                   ┃                  ┃
┃      metric       ┃   DataLoader 0    ┃   DataLoader 1    ┃   DataLoader 2   ┃
┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│     test_auc      │ 0.848648190498352 │ 0.84787690639495… │ 0.8532923460006… │
│     test_loss     │ 0.50086176395416… │ 0.49491116404533… │ 0.4625002443790… │
└───────────────────┴───────────────────┴───────────────────┴──────────────────┘
┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃ Runningstage.tes… ┃                   ┃                   ┃                  ┃
┃      metric       ┃   DataLoader 3    ┃   DataLoader 4    ┃   DataLoader 5   ┃
┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│     test_auc      │ 0.85051578283309… │ 0.84602046012878… │ 0.8423361778259… │
│     test_loss     │ 0.49442839622497… │ 0.50504952669143… │ 0.5099893212318… │
└───────────────────┴───────────────────┴───────────────────┴──────────────────┘
┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃ Runningstage.tes… ┃                   ┃                   ┃                  ┃
┃      metric       ┃   DataLoader 6    ┃   DataLoader 7    ┃   DataLoader 8   ┃
┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│     test_auc      │ 0.83244085311889… │ 0.82627463340759… │ 0.8226410150527… │
│     test_loss     │ 0.56511324644088… │ 0.55289673805236… │ 0.5619202256202… │
└───────────────────┴───────────────────┴───────────────────┴──────────────────┘
┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃ Runningstage.tes… ┃                   ┃                   ┃                  ┃
┃      metric       ┃   DataLoader 9    ┃   DataLoader 10   ┃  DataLoader 11   ┃
┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│     test_auc      │ 0.815454363822937 │ 0.80760276317596… │ 0.7975318431854… │
│     test_loss     │ 0.57133108377456… │ 0.57984864711761… │ 0.6174841523170… │
└───────────────────┴───────────────────┴───────────────────┴──────────────────┘
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                       epoch ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇█████
wandb:   test_auc/dataloader_idx_0 ▁
wandb:   test_auc/dataloader_idx_1 ▁
wandb:  test_auc/dataloader_idx_10 ▁
wandb:  test_auc/dataloader_idx_11 ▁
wandb:   test_auc/dataloader_idx_2 ▁
wandb:   test_auc/dataloader_idx_3 ▁
wandb:   test_auc/dataloader_idx_4 ▁
wandb:   test_auc/dataloader_idx_5 ▁
wandb:   test_auc/dataloader_idx_6 ▁
wandb:   test_auc/dataloader_idx_7 ▁
wandb:   test_auc/dataloader_idx_8 ▁
wandb:   test_auc/dataloader_idx_9 ▁
wandb:  test_loss/dataloader_idx_0 ▁
wandb:  test_loss/dataloader_idx_1 ▁
wandb: test_loss/dataloader_idx_10 ▁
wandb: test_loss/dataloader_idx_11 ▁
wandb:  test_loss/dataloader_idx_2 ▁
wandb:  test_loss/dataloader_idx_3 ▁
wandb:  test_loss/dataloader_idx_4 ▁
wandb:  test_loss/dataloader_idx_5 ▁
wandb:  test_loss/dataloader_idx_6 ▁
wandb:  test_loss/dataloader_idx_7 ▁
wandb:  test_loss/dataloader_idx_8 ▁
wandb:  test_loss/dataloader_idx_9 ▁
wandb:            train_loss_epoch █▆▅▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:             train_loss_step ▅▅▄▄▄▄▄▅▃▃▄▅▄▄▅▁▂▂▃▂▄▃▃▂▂▁▄▃▂▂▁█▃▂▁▃▆▆▃▄
wandb:         trainer/global_step ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███
wandb:                     val_auc ▁▂▃▄▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇███████████
wandb:                    val_loss █▇▅▅▅▅▄▄▄▅▃▅▃▃▅▄▄▃▄▄▃▃▃▃▃▂▃▄▃▃▁▃▃▃▃▃▁▃▂▃
wandb: 
wandb: Run summary:
wandb:                       epoch 40
wandb:   test_auc/dataloader_idx_0 0.84865
wandb:   test_auc/dataloader_idx_1 0.84788
wandb:  test_auc/dataloader_idx_10 0.8076
wandb:  test_auc/dataloader_idx_11 0.79753
wandb:   test_auc/dataloader_idx_2 0.85329
wandb:   test_auc/dataloader_idx_3 0.85052
wandb:   test_auc/dataloader_idx_4 0.84602
wandb:   test_auc/dataloader_idx_5 0.84234
wandb:   test_auc/dataloader_idx_6 0.83244
wandb:   test_auc/dataloader_idx_7 0.82627
wandb:   test_auc/dataloader_idx_8 0.82264
wandb:   test_auc/dataloader_idx_9 0.81545
wandb:  test_loss/dataloader_idx_0 0.50086
wandb:  test_loss/dataloader_idx_1 0.49491
wandb: test_loss/dataloader_idx_10 0.57985
wandb: test_loss/dataloader_idx_11 0.61748
wandb:  test_loss/dataloader_idx_2 0.4625
wandb:  test_loss/dataloader_idx_3 0.49443
wandb:  test_loss/dataloader_idx_4 0.50505
wandb:  test_loss/dataloader_idx_5 0.50999
wandb:  test_loss/dataloader_idx_6 0.56511
wandb:  test_loss/dataloader_idx_7 0.5529
wandb:  test_loss/dataloader_idx_8 0.56192
wandb:  test_loss/dataloader_idx_9 0.57133
wandb:            train_loss_epoch 0.48656
wandb:             train_loss_step 0.66199
wandb:         trainer/global_step 522400
wandb:                     val_auc 0.84145
wandb:                    val_loss 0.43547
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /gpfs/scratch/slj9342/wandb/wandb/offline-run-20260121_151618-enssektn
wandb: Find logs at: /gpfs/scratch/slj9342/wandb/wandb/offline-run-20260121_151618-enssektn/logs
Saved results to: /gpfs/data/oermannlab/users/slj9342/la_maml_clinical_v2/results/la_maml_clinical_v2/traditional-nonsequential-2013-2018/traditional-nonsequential-2013-2018-seed-10.csv
============================================
End time: Fri Jan 23 09:41:40 EST 2026
Job completed successfully
